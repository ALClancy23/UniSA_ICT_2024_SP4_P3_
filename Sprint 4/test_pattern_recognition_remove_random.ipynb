{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The goal of Milestone 3 is to generate code to randomly remove some data to replicate incomplete data, and then determine the accuracy of the coding to determine the opening move with missing data. This code begins by randomly generating the data removed and then applying the pattern recognition framework to assess its accuracy in identifying chess openings from incomplete game records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: 'test_pattern_recognition_v2.ipynb' contains and utilises training and testing data to complete objective. This code loops around all losses and gives accuracy for all.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries and Setup\n",
    "\n",
    "This section imports the necessary libraries for data manipulation and visualisation. \n",
    "\n",
    "The sys library is used to modify the system path to include the directory where the ChessOpeningMapper module is located.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Import ChessOpeningMapper\n",
    "from ChessOpeningMapper import ChessOpeningMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\clancyam\\Documents\\GitHub\\UniSA_ICT_2024_SP4_P3_\\Sprint 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load Opening Moves and Create Trie Structure.\n",
    "# \n",
    "In this step, an instance of ChessOpeningMapper is created.\n",
    "\n",
    "A list of file paths to the TSV files containing chess openings is defined.\n",
    "\n",
    "These TSV files are merged into a single DataFrame using merge_tsv_files.\n",
    "\n",
    "The PGN strings are split into individual moves using split_pgn_to_columns.\n",
    "\n",
    "A Trie structure is created from the opening moves using create_trie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Moves DataFrame:\n",
      "   eco                                     name  \\\n",
      "0  A00                             Amar Opening   \n",
      "1  A00               Amar Opening: Paris Gambit   \n",
      "2  A00  Amar Opening: Paris Gambit, Gent Gambit   \n",
      "3  A00                         Amsterdam Attack   \n",
      "4  A00                      Anderssen's Opening   \n",
      "\n",
      "                                                 pgn Move_ply_1 Move_ply_2  \\\n",
      "0                                             1. Nh3        Nh3       None   \n",
      "1                           1. Nh3 d5 2. g3 e5 3. f4        Nh3         d5   \n",
      "2  1. Nh3 d5 2. g3 e5 3. f4 Bxh3 4. Bxh3 exf4 5. ...        Nh3         d5   \n",
      "3             1. e3 e5 2. c4 d6 3. Nc3 Nc6 4. b3 Nf6         e3         e5   \n",
      "4                                              1. a3         a3       None   \n",
      "\n",
      "  Move_ply_3 Move_ply_4 Move_ply_5 Move_ply_6 Move_ply_7  ... Move_ply_28  \\\n",
      "0       None       None       None       None       None  ...        None   \n",
      "1         g3         e5         f4       None       None  ...        None   \n",
      "2         g3         e5         f4       Bxh3       Bxh3  ...        None   \n",
      "3         c4         d6        Nc3        Nc6         b3  ...        None   \n",
      "4       None       None       None       None       None  ...        None   \n",
      "\n",
      "  Move_ply_29 Move_ply_30 Move_ply_31 Move_ply_32 Move_ply_33 Move_ply_34  \\\n",
      "0        None        None        None        None        None        None   \n",
      "1        None        None        None        None        None        None   \n",
      "2        None        None        None        None        None        None   \n",
      "3        None        None        None        None        None        None   \n",
      "4        None        None        None        None        None        None   \n",
      "\n",
      "  Move_ply_35 Move_ply_36                                         plies  \n",
      "0        None        None                                           Nh3  \n",
      "1        None        None                               Nh3 d5 g3 e5 f4  \n",
      "2        None        None  Nh3 d5 g3 e5 f4 Bxh3 Bxh3 exf4 O-O fxg3 hxg3  \n",
      "3        None        None                    e3 e5 c4 d6 Nc3 Nc6 b3 Nf6  \n",
      "4        None        None                                            a3  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ChessOpeningMapper\n",
    "mapper = ChessOpeningMapper()\n",
    "\n",
    "# Define a list of file paths to the TSV files containing chess openings, I had issues with the path, so I have mapped them manually. \n",
    "file_list = [\n",
    "    '../Chess Pattern Recognition/a.tsv',\n",
    "    '../Chess Pattern Recognition/b.tsv',\n",
    "    '../Chess Pattern Recognition/c.tsv',\n",
    "    '../Chess Pattern Recognition/d.tsv',\n",
    "    '../Chess Pattern Recognition/e.tsv'\n",
    "]\n",
    "\n",
    "# Merge the TSV files into a single DataFrame\n",
    "opening_moves = mapper.merge_tsv_files(file_list)\n",
    "\n",
    "# Split the PGN strings into individual moves\n",
    "opening_moves = mapper.split_pgn_to_columns(opening_moves)\n",
    "\n",
    "# Create a Trie structure from the opening moves\n",
    "mapper.create_trie(opening_moves)\n",
    "\n",
    "# Display the first few rows of the opening moves DataFrame\n",
    "print(\"Opening Moves DataFrame:\")\n",
    "print(opening_moves.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Unzip and Load Chess Game Data\n",
    "This step involves:\n",
    "\n",
    "Defining the path to the zipped game data file.\n",
    "\n",
    "Unzipping the game data file to extract the CSV file.\n",
    "\n",
    "Loading the extracted CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\clancyam\\Documents\\GitHub\\UniSA_ICT_2024_SP4_P3_\\Sprint 4\\ChessOpeningMapper.py:152: DtypeWarning: Columns (188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  game_data = pd.read_csv(extracted_file_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Data DataFrame:\n",
      "   Index        Date  ECO                                 Opening Result  \\\n",
      "0      0  2019.04.30  B15                       Caro-Kann Defense    0-1   \n",
      "1      1  2019.04.30  C50                            Italian Game    0-1   \n",
      "2      2  2019.04.30  C41                     Philidor Defense #2    1-0   \n",
      "3      3  2019.04.30  B06                          Modern Defense    0-1   \n",
      "4      4  2019.04.30  B32  Sicilian Defense: Loewenthal Variation    1-0   \n",
      "\n",
      "  Termination TimeControl     UTCDate   UTCTime Move_ply_1  ... Clock_ply_192  \\\n",
      "0      Normal       300+3  2019.04.30  22:00:24         d4  ...           NaN   \n",
      "1      Normal       300+0  2019.04.30  22:00:13         e4  ...           NaN   \n",
      "2      Normal       600+0  2019.04.30  22:00:41         e4  ...           NaN   \n",
      "3      Normal        60+0  2019.04.30  22:00:43         e4  ...           NaN   \n",
      "4      Normal       180+0  2019.04.30  22:00:46         e4  ...           NaN   \n",
      "\n",
      "  Clock_ply_193 Clock_ply_194 Clock_ply_195 Clock_ply_196 Clock_ply_197  \\\n",
      "0           NaN           NaN           NaN           NaN           NaN   \n",
      "1           NaN           NaN           NaN           NaN           NaN   \n",
      "2           NaN           NaN           NaN           NaN           NaN   \n",
      "3           NaN           NaN           NaN           NaN           NaN   \n",
      "4           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "  Clock_ply_198 Clock_ply_199 Clock_ply_200 Category  \n",
      "0           NaN           NaN           NaN    Blitz  \n",
      "1           NaN           NaN           NaN    Blitz  \n",
      "2           NaN           NaN           NaN    Rapid  \n",
      "3           NaN           NaN           NaN   Bullet  \n",
      "4           NaN           NaN           NaN    Blitz  \n",
      "\n",
      "[5 rows x 410 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the zipped game data file\n",
    "game_data_zip_path = '../Chess Pattern Recognition/chessdata.zip'\n",
    "\n",
    "# Define the name of the extracted CSV file\n",
    "extracted_file_name = 'chessdata.csv'\n",
    "\n",
    "# Unzip the game data file\n",
    "ChessOpeningMapper.unzip_game_data(zip_path=game_data_zip_path, extract_to='.')\n",
    "\n",
    "# Load the extracted CSV file into a DataFrame\n",
    "game_data = ChessOpeningMapper.load_game_data(zip_path=game_data_zip_path, extracted_file_name=extracted_file_name)\n",
    "\n",
    "# Display the first few rows of the game data DataFrame\n",
    "print(\"Game Data DataFrame:\")\n",
    "print(game_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Map Opening Names to Game Data\n",
    "\n",
    "Here:\n",
    "\n",
    "The game data is processed to map the move sequences to opening names using get_opening_name_from_game.\n",
    "\n",
    "The mapped opening names are added to the original game data DataFrame in a new column called mapped_opening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows with mapped openings: \n",
      "   Index        Date  ECO                                 Opening Result  \\\n",
      "0      0  2019.04.30  B15                       Caro-Kann Defense    0-1   \n",
      "1      1  2019.04.30  C50                            Italian Game    0-1   \n",
      "2      2  2019.04.30  C41                     Philidor Defense #2    1-0   \n",
      "3      3  2019.04.30  B06                          Modern Defense    0-1   \n",
      "4      4  2019.04.30  B32  Sicilian Defense: Loewenthal Variation    1-0   \n",
      "\n",
      "  Termination TimeControl     UTCDate   UTCTime Move_ply_1  ... Clock_ply_193  \\\n",
      "0      Normal       300+3  2019.04.30  22:00:24         d4  ...           NaN   \n",
      "1      Normal       300+0  2019.04.30  22:00:13         e4  ...           NaN   \n",
      "2      Normal       600+0  2019.04.30  22:00:41         e4  ...           NaN   \n",
      "3      Normal        60+0  2019.04.30  22:00:43         e4  ...           NaN   \n",
      "4      Normal       180+0  2019.04.30  22:00:46         e4  ...           NaN   \n",
      "\n",
      "  Clock_ply_194 Clock_ply_195 Clock_ply_196 Clock_ply_197 Clock_ply_198  \\\n",
      "0           NaN           NaN           NaN           NaN           NaN   \n",
      "1           NaN           NaN           NaN           NaN           NaN   \n",
      "2           NaN           NaN           NaN           NaN           NaN   \n",
      "3           NaN           NaN           NaN           NaN           NaN   \n",
      "4           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "  Clock_ply_199 Clock_ply_200 Category                         mapped_opening  \n",
      "0           NaN           NaN    Blitz  Queen's Pawn Game: Chigorin Variation  \n",
      "1           NaN           NaN    Blitz            Italian Game: Paris Defense  \n",
      "2           NaN           NaN    Rapid                       Philidor Defense  \n",
      "3           NaN           NaN   Bullet                         Modern Defense  \n",
      "4           NaN           NaN    Blitz  Sicilian Defense: Löwenthal Variation  \n",
      "\n",
      "[5 rows x 411 columns]\n"
     ]
    }
   ],
   "source": [
    "# Map the opening names to the game data\n",
    "result_df = mapper.get_opening_name_from_game(game_data)\n",
    "\n",
    "# Add the mapped opening names to the original game data DataFrame\n",
    "game_data['mapped_opening'] = result_df['opening_name']\n",
    "\n",
    "# Display the first 5 rows of the updated game data DataFrame\n",
    "print(f\"First 5 rows with mapped openings: \\n{game_data.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train/Test Split\n",
    "\n",
    "Here: \n",
    "\n",
    "Split the dataset into training and test sets with a 70/30 ratio. Display the sizes of the train and test datasets to verify the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 139777\n",
      "Testing data size: 59905\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train/Test Split\n",
    "\n",
    "# Filter out openings that appear less than twice\n",
    "value_counts = game_data['mapped_opening'].value_counts()\n",
    "to_keep = value_counts[value_counts > 1].index\n",
    "game_data = game_data[game_data['mapped_opening'].isin(to_keep)]\n",
    "\n",
    "# Split the dataset into training and test sets (70/30)\n",
    "train_df, test_df = train_test_split(game_data, test_size=0.3, random_state=42,stratify=game_data['mapped_opening'])\n",
    "\n",
    "# Display the sizes of the train and test datasets\n",
    "print(f\"Training data size: {len(train_df)}\")\n",
    "print(f\"Testing data size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Distribution:\n",
      " mapped_opening\n",
      "Queen's Pawn Game                                                 0.025491\n",
      "Horwitz Defense                                                   0.021334\n",
      "Philidor Defense                                                  0.019336\n",
      "Queen's Pawn Game: Accelerated London System                      0.018444\n",
      "Van't Kruijs Opening                                              0.016712\n",
      "                                                                    ...   \n",
      "Queen's Gambit Accepted: Alekhine Defense, Haberditz Variation    0.000010\n",
      "Borg Defense: Zilbermints Gambit                                  0.000010\n",
      "King's Gambit Accepted: Bishop's Gambit, Lopez Variation          0.000010\n",
      "Semi-Slav Defense: Marshall Gambit, Main Line                     0.000010\n",
      "English Opening: King's English Variation, Taimanov Variation     0.000010\n",
      "Name: proportion, Length: 1642, dtype: float64\n",
      "Training Distribution:\n",
      " mapped_opening\n",
      "Queen's Pawn Game                                                0.025491\n",
      "Horwitz Defense                                                  0.021334\n",
      "Philidor Defense                                                 0.019338\n",
      "Queen's Pawn Game: Accelerated London System                     0.018444\n",
      "Van't Kruijs Opening                                             0.016712\n",
      "                                                                   ...   \n",
      "Italian Game: Evans Gambit, Leonhardt Countergambit              0.000007\n",
      "Sicilian Defense: Grand Prix Attack, Schofman Variation          0.000007\n",
      "Ruy Lopez: Marshall Attack, Original Marshall Attack             0.000007\n",
      "Goldsmith Defense: Picklepuss Defense                            0.000007\n",
      "English Opening: King's English Variation, Taimanov Variation    0.000007\n",
      "Name: proportion, Length: 1642, dtype: float64\n",
      "Testing Distribution:\n",
      " mapped_opening\n",
      "Queen's Pawn Game                                                                             0.025490\n",
      "Horwitz Defense                                                                               0.021334\n",
      "Philidor Defense                                                                              0.019331\n",
      "Queen's Pawn Game: Accelerated London System                                                  0.018446\n",
      "Van't Kruijs Opening                                                                          0.016710\n",
      "                                                                                                ...   \n",
      "English Opening: King's English Variation, Four Knights Variation, Bradley Beach Variation    0.000017\n",
      "Indian Defense: Gibbins-Weidenhagen Gambit Accepted                                           0.000017\n",
      "Alekhine Defense: Four Pawns Attack, Trifunovic Variation                                     0.000017\n",
      "Benko Gambit Declined: Hjørring Countergambit                                                 0.000017\n",
      "Queen's Gambit Accepted: Mannheim Variation                                                   0.000017\n",
      "Name: proportion, Length: 1642, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Distribution:\\n\", game_data['mapped_opening'].value_counts(normalize=True))\n",
    "print(\"Training Distribution:\\n\", train_df['mapped_opening'].value_counts(normalize=True))\n",
    "print(\"Testing Distribution:\\n\", test_df['mapped_opening'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generate Random Code\n",
    "\n",
    "Here:\n",
    "\n",
    "A method is defined to generate a random percentage of data poiints across the move plies to be removed\n",
    "\n",
    "it returns the modified dataset with the removed values and a dictionary of the lost datapoints\n",
    "\n",
    "It randomly removes data points by taking the raw data, the percentage wish to be removed and the total plies across the percentage of moves to be removed\n",
    "\n",
    "It then randomly picks a row in the dataset and a corresponding column that is the move plies within the set limit and forms the indice to be removed.\n",
    "\n",
    "The current test only works on 1% of the data across 50 plies to minimise compute time while testing.\n",
    "it the prints out the total number of row indices removed from each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of training data after random move removal:\n",
      "         Index        Date  ECO  \\\n",
      "160507  160507  2019.05.17  C45   \n",
      "156024  156024  2019.05.17  C45   \n",
      "42935    42935  2019.05.03  B12   \n",
      "29007    29007  2019.05.27  B10   \n",
      "172373  172373  2019.05.18  C55   \n",
      "\n",
      "                                                  Opening Result  \\\n",
      "160507                                        Scotch Game    0-1   \n",
      "156024                   Scotch Game: Classical Variation    0-1   \n",
      "42935   Caro-Kann Defense: Advance Variation, Short Va...    1-0   \n",
      "29007                                   Caro-Kann Defense    1-0   \n",
      "172373  Italian Game: Two Knights Defense, Max Lange A...    1-0   \n",
      "\n",
      "         Termination TimeControl     UTCDate   UTCTime Move_ply_1  ...  \\\n",
      "160507        Normal      900+15  2019.05.17  19:16:57         e4  ...   \n",
      "156024        Normal       180+0  2019.05.17   2:48:19         e4  ...   \n",
      "42935   Time forfeit       180+0  2019.05.03  23:42:29         e4  ...   \n",
      "29007         Normal      900+15  2019.05.27   0:51:54         e4  ...   \n",
      "172373  Time forfeit       300+3  2019.05.18  14:03:10         e4  ...   \n",
      "\n",
      "       Clock_ply_193 Clock_ply_194 Clock_ply_195 Clock_ply_196 Clock_ply_197  \\\n",
      "160507           NaN           NaN           NaN           NaN           NaN   \n",
      "156024           NaN           NaN           NaN           NaN           NaN   \n",
      "42935            NaN           NaN           NaN           NaN           NaN   \n",
      "29007            NaN           NaN           NaN           NaN           NaN   \n",
      "172373           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "       Clock_ply_198 Clock_ply_199 Clock_ply_200   Category  \\\n",
      "160507           NaN           NaN           NaN  Classical   \n",
      "156024           NaN           NaN           NaN      Blitz   \n",
      "42935            NaN           NaN           NaN      Blitz   \n",
      "29007            NaN           NaN           NaN  Classical   \n",
      "172373           NaN           NaN           NaN      Blitz   \n",
      "\n",
      "                                           mapped_opening  \n",
      "160507                                        Scotch Game  \n",
      "156024                   Scotch Game: Classical Variation  \n",
      "42935   Caro-Kann Defense: Advance Variation, Short Va...  \n",
      "29007                                   Caro-Kann Defense  \n",
      "172373                         Italian Game: Deutz Gambit  \n",
      "\n",
      "[5 rows x 411 columns]\n",
      "Sample of testing data after random move removal:\n",
      "         Index        Date  ECO  \\\n",
      "76403    76403  2019.05.01  C62   \n",
      "48651    48651  2019.05.28  D31   \n",
      "100696  100696  2019.05.08  C15   \n",
      "2971      2971  2019.05.22  E10   \n",
      "110570  110570  2019.05.09  C00   \n",
      "\n",
      "                                                  Opening Result  \\\n",
      "76403                         Ruy Lopez: Steinitz Defense    0-1   \n",
      "48651   Queen's Gambit Declined: Queen's Knight Variation    0-1   \n",
      "100696                  French Defense: Winawer Variation    1-0   \n",
      "2971                       Indian Game: Anti-Nimzo-Indian    1-0   \n",
      "110570                   French Defense: Knight Variation    1-0   \n",
      "\n",
      "         Termination TimeControl     UTCDate   UTCTime Move_ply_1  ...  \\\n",
      "76403   Time forfeit       180+2  2019.05.01  16:39:19         e4  ...   \n",
      "48651   Time forfeit       180+0  2019.05.28  21:49:23         c4  ...   \n",
      "100696        Normal      900+15  2019.05.08  13:52:10         e4  ...   \n",
      "2971          Normal       180+2  2019.05.22  22:20:22         d4  ...   \n",
      "110570  Time forfeit      180+15  2019.05.09  23:53:25         e4  ...   \n",
      "\n",
      "       Clock_ply_193 Clock_ply_194 Clock_ply_195 Clock_ply_196 Clock_ply_197  \\\n",
      "76403            NaN           NaN           NaN           NaN           NaN   \n",
      "48651            NaN           NaN           NaN           NaN           NaN   \n",
      "100696           NaN           NaN           NaN           NaN           NaN   \n",
      "2971             NaN           NaN           NaN           NaN           NaN   \n",
      "110570           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "       Clock_ply_198 Clock_ply_199 Clock_ply_200   Category  \\\n",
      "76403            NaN           NaN           NaN      Blitz   \n",
      "48651            NaN           NaN           NaN      Blitz   \n",
      "100696           NaN           NaN           NaN  Classical   \n",
      "2971             NaN           NaN           NaN      Blitz   \n",
      "110570           NaN           NaN           NaN      Rapid   \n",
      "\n",
      "                            mapped_opening  \n",
      "76403          Ruy Lopez: Steinitz Defense  \n",
      "48651   English Opening: Agincourt Defense  \n",
      "100696   French Defense: Winawer Variation  \n",
      "2971     Indian Defense: Anti-Nimzo-Indian  \n",
      "110570    French Defense: Knight Variation  \n",
      "\n",
      "[5 rows x 411 columns]\n",
      "Column 90: 1391 rows removed\n",
      "Column 22: 1404 rows removed\n",
      "Column 60: 1415 rows removed\n",
      "Column 96: 1358 rows removed\n",
      "Column 99: 1320 rows removed\n",
      "Column 16: 1449 rows removed\n",
      "Column 83: 1408 rows removed\n",
      "Column 46: 1422 rows removed\n",
      "Column 81: 1333 rows removed\n",
      "Column 4: 1338 rows removed\n",
      "Column 93: 1376 rows removed\n",
      "Column 48: 1441 rows removed\n",
      "Column 56: 1300 rows removed\n",
      "Column 75: 1445 rows removed\n",
      "Column 64: 1420 rows removed\n",
      "Column 66: 1338 rows removed\n",
      "Column 40: 1405 rows removed\n",
      "Column 38: 1501 rows removed\n",
      "Column 59: 1422 rows removed\n",
      "Column 25: 1415 rows removed\n",
      "Column 72: 1340 rows removed\n",
      "Column 28: 1387 rows removed\n",
      "Column 19: 1499 rows removed\n",
      "Column 49: 1426 rows removed\n",
      "Column 9: 1417 rows removed\n",
      "Column 82: 1407 rows removed\n",
      "Column 85: 1456 rows removed\n",
      "Column 80: 1448 rows removed\n",
      "Column 86: 1409 rows removed\n",
      "Column 73: 1342 rows removed\n",
      "Column 35: 1463 rows removed\n",
      "Column 67: 1389 rows removed\n",
      "Column 71: 1411 rows removed\n",
      "Column 20: 1406 rows removed\n",
      "Column 2: 1397 rows removed\n",
      "Column 41: 1428 rows removed\n",
      "Column 52: 1356 rows removed\n",
      "Column 92: 1392 rows removed\n",
      "Column 34: 1416 rows removed\n",
      "Column 87: 1383 rows removed\n",
      "Column 1: 1364 rows removed\n",
      "Column 94: 1389 rows removed\n",
      "Column 10: 1392 rows removed\n",
      "Column 74: 1407 rows removed\n",
      "Column 32: 1403 rows removed\n",
      "Column 50: 1432 rows removed\n",
      "Column 79: 1411 rows removed\n",
      "Column 15: 1426 rows removed\n",
      "Column 14: 1326 rows removed\n",
      "Column 45: 1395 rows removed\n",
      "Column 33: 1372 rows removed\n",
      "Column 27: 1373 rows removed\n",
      "Column 78: 1330 rows removed\n",
      "Column 95: 1414 rows removed\n",
      "Column 42: 1391 rows removed\n",
      "Column 39: 1490 rows removed\n",
      "Column 12: 1397 rows removed\n",
      "Column 29: 1382 rows removed\n",
      "Column 91: 1447 rows removed\n",
      "Column 54: 1387 rows removed\n",
      "Column 26: 1388 rows removed\n",
      "Column 51: 1360 rows removed\n",
      "Column 6: 1346 rows removed\n",
      "Column 65: 1414 rows removed\n",
      "Column 13: 1406 rows removed\n",
      "Column 98: 1393 rows removed\n",
      "Column 37: 1309 rows removed\n",
      "Column 70: 1389 rows removed\n",
      "Column 84: 1395 rows removed\n",
      "Column 18: 1423 rows removed\n",
      "Column 43: 1325 rows removed\n",
      "Column 97: 1444 rows removed\n",
      "Column 5: 1417 rows removed\n",
      "Column 0: 1346 rows removed\n",
      "Column 3: 1386 rows removed\n",
      "Column 44: 1400 rows removed\n",
      "Column 31: 1449 rows removed\n",
      "Column 24: 1353 rows removed\n",
      "Column 23: 1347 rows removed\n",
      "Column 36: 1387 rows removed\n",
      "Column 63: 1436 rows removed\n",
      "Column 17: 1389 rows removed\n",
      "Column 47: 1393 rows removed\n",
      "Column 55: 1410 rows removed\n",
      "Column 68: 1440 rows removed\n",
      "Column 8: 1368 rows removed\n",
      "Column 7: 1398 rows removed\n",
      "Column 76: 1441 rows removed\n",
      "Column 69: 1371 rows removed\n",
      "Column 21: 1380 rows removed\n",
      "Column 62: 1361 rows removed\n",
      "Column 58: 1379 rows removed\n",
      "Column 57: 1457 rows removed\n",
      "Column 88: 1400 rows removed\n",
      "Column 89: 1429 rows removed\n",
      "Column 61: 1395 rows removed\n",
      "Column 11: 1419 rows removed\n",
      "Column 53: 1440 rows removed\n",
      "Column 77: 1403 rows removed\n",
      "Column 30: 1390 rows removed\n"
     ]
    }
   ],
   "source": [
    "# Generate Random Code and Apply to Both Datasets\n",
    "def generate_random_data_removal(data, percent_remove=5, total_plies=100):\n",
    "    \"\"\"\n",
    "    Randomly removes a specified percentage of data points from the first 'total_plies' columns of a DataFrame,\n",
    "    and returns a dictionary of the indices removed.\n",
    "\n",
    "    Args:\n",
    "    - data (pd.DataFrame): The DataFrame from which to remove data.\n",
    "    - percent_remove (int): The percentage of data points to remove.\n",
    "    - total_plies (int): The total number of plies to consider for data removal.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with randomly removed data points.\n",
    "    - dict: A dictionary containing the indices of the data points that were removed.\n",
    "    \"\"\"\n",
    "    modified_data = data.copy()\n",
    "    loss_dict = {}\n",
    "\n",
    "    # Calculate the total number of data points to remove\n",
    "    num_data_points = int((total_plies * len(data)) * (percent_remove / 100))\n",
    "    \n",
    "    # Generate random row and column indices to set to None\n",
    "    row_indices = np.random.randint(0, len(data), num_data_points)\n",
    "    col_indices = np.random.randint(0, total_plies, num_data_points)  \n",
    "    \n",
    "    # Set the selected data points to None and record the indices in loss_dict\n",
    "    for row, col in zip(row_indices, col_indices):\n",
    "        modified_data.iloc[row, col] = None\n",
    "        if col in loss_dict:\n",
    "            loss_dict[col].append(row)\n",
    "        else:\n",
    "            loss_dict[col] = [row]\n",
    "\n",
    "\n",
    "\n",
    "    return modified_data, loss_dict\n",
    "\n",
    "\n",
    "\n",
    "# Apply random move removal to training and testing datasets for all losses\n",
    "train_datasets, train_loss_dict = generate_random_data_removal(train_df,percent_remove=1,total_plies = 100)\n",
    "test_datasets, test_loss_dict = generate_random_data_removal(test_df,percent_remove=1, total_plies=100)\n",
    "\n",
    "print(\"Sample of training data after random move removal:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"Sample of testing data after random move removal:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# test example of 1% of data removed across 50 plies of the training data\n",
    "for column, rows in train_loss_dict.items():\n",
    "    print(f\"Column {column}: {len(rows)} rows removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([90, 22, 60, 96, 99, 16, 83, 46, 81, 4, 93, 48, 56, 75, 64, 66, 40, 38, 59, 25, 72, 28, 19, 49, 9, 82, 85, 80, 86, 73, 35, 67, 71, 20, 2, 41, 52, 92, 34, 87, 1, 94, 10, 74, 32, 50, 79, 15, 14, 45, 33, 27, 78, 95, 42, 39, 12, 29, 91, 54, 26, 51, 6, 65, 13, 98, 37, 70, 84, 18, 43, 97, 5, 0, 3, 44, 31, 24, 23, 36, 63, 17, 47, 55, 68, 8, 7, 76, 69, 21, 62, 58, 57, 88, 89, 61, 11, 53, 77, 30])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6.1 Testing**\n",
    "This tests that the data that has been removed is exclusivly from the plies columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values removed from Column Move_ply_91:\n",
      "73566     Ke2\n",
      "62536     NaN\n",
      "124580    NaN\n",
      "36550     NaN\n",
      "28856     NaN\n",
      "122710    NaN\n",
      "53001     NaN\n",
      "38378     NaN\n",
      "81484     NaN\n",
      "124236    NaN\n",
      "30909     NaN\n",
      "8373      NaN\n",
      "108533    NaN\n",
      "42300      c4\n",
      "24513     NaN\n",
      "36018     NaN\n",
      "77927     NaN\n",
      "34004     NaN\n",
      "90307     NaN\n",
      "118039    NaN\n",
      "Name: Move_ply_91, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def print_removed_values(data, loss_dict, max_samples=20):\n",
    "    total_displayed = 0  # Counter to track how many values have been displayed\n",
    "\n",
    "    for column, rows in loss_dict.items():\n",
    "        column_name = f'Move_ply_{column+1}'  \n",
    "        if len(rows) > 0:\n",
    "            original_values = data.loc[rows, column_name]  \n",
    "            sample_size = min(len(rows), max_samples - total_displayed)  \n",
    "            print(f\"Original values removed from Column {column_name}:\")\n",
    "            print(original_values.sample(sample_size))  \n",
    "\n",
    "            total_displayed += sample_size  # Update the counter\n",
    "            if total_displayed >= max_samples:\n",
    "                break  # Stop if the maximum number of samples has been reached\n",
    "\n",
    "# Example usage\n",
    "print_removed_values(game_data, train_loss_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Prepare Data for Testing\n",
    "\n",
    "In this step, game data is processed to create sequences from non-null moves, preparing it for pattern recognition testing. This step ensures each game's moves are consolidated into a format suitable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data by creating move sequences from non-null moves\n",
    "def prepare_game_data_for_testing(game_data, max_plies=200):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare the game data by creating move sequences from non-null moves.\n",
    "    \n",
    "    Args:\n",
    "    - game_data (pd.DataFrame): The DataFrame with chess moves.\n",
    "    - max_plies (int): The maximum number of plies to consider (default is 200).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with a new column 'move_sequence' containing sequences of moves.\n",
    "    \"\"\"\n",
    "    \n",
    "    move_columns = [f'Move_ply_{i+1}' for i in range(max_plies)]\n",
    "    selected_moves_df = game_data[move_columns].copy()\n",
    "\n",
    "    def create_move_sequence(row):\n",
    "        moves = row.dropna().tolist()  # Drop NaN values and convert to list\n",
    "        return ' '.join(moves)\n",
    "\n",
    "    selected_moves_df['move_sequence'] = selected_moves_df.apply(create_move_sequence, axis=1)\n",
    "    selected_moves_df['mapped_opening'] = game_data['mapped_opening']  \n",
    "    return selected_moves_df\n",
    "\n",
    "# Prepare the incomplete game data for testing\n",
    "# For simplicity, let's use the first modified dataset for further processing\n",
    "processed_train_data_sample = prepare_game_data_for_testing(train_datasets)\n",
    "processed_test_data_sample = prepare_game_data_for_testing(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Move_ply_1</th>\n",
       "      <th>Move_ply_2</th>\n",
       "      <th>Move_ply_3</th>\n",
       "      <th>Move_ply_4</th>\n",
       "      <th>Move_ply_5</th>\n",
       "      <th>Move_ply_6</th>\n",
       "      <th>Move_ply_7</th>\n",
       "      <th>Move_ply_8</th>\n",
       "      <th>Move_ply_9</th>\n",
       "      <th>Move_ply_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Move_ply_193</th>\n",
       "      <th>Move_ply_194</th>\n",
       "      <th>Move_ply_195</th>\n",
       "      <th>Move_ply_196</th>\n",
       "      <th>Move_ply_197</th>\n",
       "      <th>Move_ply_198</th>\n",
       "      <th>Move_ply_199</th>\n",
       "      <th>Move_ply_200</th>\n",
       "      <th>move_sequence</th>\n",
       "      <th>mapped_opening</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76403</th>\n",
       "      <td>e4</td>\n",
       "      <td>e5</td>\n",
       "      <td>Nf3</td>\n",
       "      <td>Nc6</td>\n",
       "      <td>Bb5</td>\n",
       "      <td>d6</td>\n",
       "      <td>Bxc6+</td>\n",
       "      <td>bxc6</td>\n",
       "      <td>d4</td>\n",
       "      <td>exd4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e4 e5 Nf3 Nc6 Bb5 d6 Bxc6+ bxc6 d4 exd4 Nxd4 N...</td>\n",
       "      <td>Ruy Lopez: Steinitz Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48651</th>\n",
       "      <td>c4</td>\n",
       "      <td>e6</td>\n",
       "      <td>Nc3</td>\n",
       "      <td>d5</td>\n",
       "      <td>d4</td>\n",
       "      <td>dxc4</td>\n",
       "      <td>e4</td>\n",
       "      <td>Nf6</td>\n",
       "      <td>Bxc4</td>\n",
       "      <td>c5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c4 e6 Nc3 d5 d4 dxc4 e4 Nf6 Bxc4 c5 d5 exd5 ex...</td>\n",
       "      <td>English Opening: Agincourt Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100696</th>\n",
       "      <td>e4</td>\n",
       "      <td>e6</td>\n",
       "      <td>d4</td>\n",
       "      <td>d5</td>\n",
       "      <td>Nc3</td>\n",
       "      <td>Bb4</td>\n",
       "      <td>Bd3</td>\n",
       "      <td>Nf6</td>\n",
       "      <td>e5</td>\n",
       "      <td>Nfd7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e4 e6 d4 d5 Nc3 Bb4 Bd3 Nf6 e5 Nfd7 Qg4 O-O Bh...</td>\n",
       "      <td>French Defense: Winawer Variation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>d4</td>\n",
       "      <td>Nf6</td>\n",
       "      <td>c4</td>\n",
       "      <td>e6</td>\n",
       "      <td>Nf3</td>\n",
       "      <td>d5</td>\n",
       "      <td>cxd5</td>\n",
       "      <td>exd5</td>\n",
       "      <td>Bg5</td>\n",
       "      <td>c6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d4 Nf6 c4 e6 Nf3 d5 cxd5 exd5 Bg5 c6 e3 Bd6 Nc...</td>\n",
       "      <td>Indian Defense: Anti-Nimzo-Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110570</th>\n",
       "      <td>e4</td>\n",
       "      <td>e6</td>\n",
       "      <td>Nf3</td>\n",
       "      <td>b6</td>\n",
       "      <td>d4</td>\n",
       "      <td>Bb7</td>\n",
       "      <td>Bd3</td>\n",
       "      <td>f5</td>\n",
       "      <td>Qe2</td>\n",
       "      <td>Nf6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e4 e6 Nf3 b6 d4 Bb7 Bd3 f5 Qe2 Nf6 exf5 Bd6 fx...</td>\n",
       "      <td>French Defense: Knight Variation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Move_ply_1 Move_ply_2 Move_ply_3 Move_ply_4 Move_ply_5 Move_ply_6  \\\n",
       "76403          e4         e5        Nf3        Nc6        Bb5         d6   \n",
       "48651          c4         e6        Nc3         d5         d4       dxc4   \n",
       "100696         e4         e6         d4         d5        Nc3        Bb4   \n",
       "2971           d4        Nf6         c4         e6        Nf3         d5   \n",
       "110570         e4         e6        Nf3         b6         d4        Bb7   \n",
       "\n",
       "       Move_ply_7 Move_ply_8 Move_ply_9 Move_ply_10  ... Move_ply_193  \\\n",
       "76403       Bxc6+       bxc6         d4        exd4  ...          NaN   \n",
       "48651          e4        Nf6       Bxc4          c5  ...          NaN   \n",
       "100696        Bd3        Nf6         e5        Nfd7  ...          NaN   \n",
       "2971         cxd5       exd5        Bg5          c6  ...          NaN   \n",
       "110570        Bd3         f5        Qe2         Nf6  ...          NaN   \n",
       "\n",
       "       Move_ply_194 Move_ply_195 Move_ply_196 Move_ply_197 Move_ply_198  \\\n",
       "76403           NaN          NaN          NaN          NaN          NaN   \n",
       "48651           NaN          NaN          NaN          NaN          NaN   \n",
       "100696          NaN          NaN          NaN          NaN          NaN   \n",
       "2971            NaN          NaN          NaN          NaN          NaN   \n",
       "110570          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "       Move_ply_199 Move_ply_200  \\\n",
       "76403           NaN          NaN   \n",
       "48651           NaN          NaN   \n",
       "100696          NaN          NaN   \n",
       "2971            NaN          NaN   \n",
       "110570          NaN          NaN   \n",
       "\n",
       "                                            move_sequence  \\\n",
       "76403   e4 e5 Nf3 Nc6 Bb5 d6 Bxc6+ bxc6 d4 exd4 Nxd4 N...   \n",
       "48651   c4 e6 Nc3 d5 d4 dxc4 e4 Nf6 Bxc4 c5 d5 exd5 ex...   \n",
       "100696  e4 e6 d4 d5 Nc3 Bb4 Bd3 Nf6 e5 Nfd7 Qg4 O-O Bh...   \n",
       "2971    d4 Nf6 c4 e6 Nf3 d5 cxd5 exd5 Bg5 c6 e3 Bd6 Nc...   \n",
       "110570  e4 e6 Nf3 b6 d4 Bb7 Bd3 f5 Qe2 Nf6 exf5 Bd6 fx...   \n",
       "\n",
       "                            mapped_opening  \n",
       "76403          Ruy Lopez: Steinitz Defense  \n",
       "48651   English Opening: Agincourt Defense  \n",
       "100696   French Defense: Winawer Variation  \n",
       "2971     Indian Defense: Anti-Nimzo-Indian  \n",
       "110570    French Defense: Knight Variation  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Test Pattern Recognition on Incomplete Data\n",
    "\n",
    "In this step, the ChessOpeningMapper is used to predict chess openings from the processed, incomplete data. The predicted openings are then compared with the actual mapped openings to determine the accuracy of the model, assessing its ability to handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on incomplete training data: 96.11%\n",
      "Accuracy on incomplete testing data: 96.09%\n"
     ]
    }
   ],
   "source": [
    "# Test pattern recognition on incomplete data\n",
    "def test_pattern_recognition_on_incomplete_data(mapper, incomplete_df, dataset_label):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test the pattern recognition on incomplete data using ChessOpeningMapper.\n",
    "    \n",
    "    Args:\n",
    "    - mapper (ChessOpeningMapper): The ChessOpeningMapper instance for mapping openings.\n",
    "    - incomplete_df (pd.DataFrame): The DataFrame containing incomplete chess move data.\n",
    "    - dataset_label (str): Label to indicate which dataset is being tested (e.g., 'Training' or 'Testing').\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with the original and predicted openings for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the mapper to predict openings based on incomplete data\n",
    "    incomplete_results = mapper.get_opening_name_from_game(incomplete_df)\n",
    "    incomplete_df['predicted_opening'] = incomplete_results['opening_name']\n",
    "    \n",
    "    if 'mapped_opening' in incomplete_df.columns:\n",
    "        incomplete_df['match'] = incomplete_df['mapped_opening'] == incomplete_df['predicted_opening']\n",
    "        accuracy = incomplete_df['match'].mean()\n",
    "        print(f'Accuracy on incomplete {dataset_label} data: {accuracy:.2%}')\n",
    "    else:\n",
    "        print(f\"No 'mapped_opening' column found in input {dataset_label} data\")\n",
    "    \n",
    "    return incomplete_df\n",
    "\n",
    "# Instantiate ChessOpeningMapper and load trie\n",
    "mapper = ChessOpeningMapper()\n",
    "opening_moves = mapper.load_openings()\n",
    "mapper.create_trie(opening_moves)\n",
    "\n",
    "# Run the test on both training and testing data\n",
    "train_result_df_test = test_pattern_recognition_on_incomplete_data(mapper, processed_train_data_sample, \"training\")\n",
    "test_result_df_test = test_pattern_recognition_on_incomplete_data(mapper, processed_test_data_sample, \"testing\")\n",
    "\n",
    "\n",
    "# Print results for training data\n",
    "#print(\"\\nTraining Data Results:\")\n",
    "#print(\"-\" * 50)\n",
    "#if 'mapped_opening' in train_result_df.columns and 'match' in train_result_df.columns:\n",
    " #   print(train_result_df[['mapped_opening', 'predicted_opening', 'match']].head())\n",
    "#else:\n",
    "#    print(\"Mapped or Match columns not found in the train result DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test pattern recognition on all losses for both train and test datasets\n",
    "\n",
    "# # Initialise a dictionary to store results for each loss scenario\n",
    "# results = {}\n",
    "\n",
    "# # Loop through each loss scenario in the training loss dictionary\n",
    "# # `train_loss_dict.keys()` contains keys like 'loss1', 'loss2', ..., 'loss30'\n",
    "# for  loss in train_loss_dict.keys():\n",
    "    \n",
    "#     # Prepare the training data by creating sequences of moves for the current loss scenario\n",
    "#     # `train_datasets[idx]` contains the modified training dataset for the current loss\n",
    "#     processed_train_data = prepare_game_data_for_testing(train_datasets[loss])\n",
    "\n",
    "#     # Prepare the testing data by creating sequences of moves for the current loss scenario\n",
    "#     # `test_datasets[idx]` contains the modified testing dataset for the current loss\n",
    "#     processed_test_data = prepare_game_data_for_testing(test_datasets[loss])\n",
    "    \n",
    "#     # Test pattern recognition on the processed training data for the current loss\n",
    "#     # This function will predict the openings and compare with actual mapped openings\n",
    "#     train_result_df = test_pattern_recognition_on_incomplete_data(mapper, processed_train_data, f\"training ({loss})\")\n",
    "\n",
    "#     # Test pattern recognition on the processed testing data for the current loss\n",
    "#     test_result_df = test_pattern_recognition_on_incomplete_data(mapper, processed_test_data, f\"testing ({loss})\")\n",
    "    \n",
    "#     # Store the result dataframes (train and test) in the results dictionary under the current loss key\n",
    "#     results[loss] = {\n",
    "#         'train': train_result_df, # Result DataFrame for training data\n",
    "#         'test': test_result_df # Result DataFrame for testing data\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on incomplete training (90) data: 92.35%\n",
      "Accuracy on incomplete testing (90) data: 92.41%\n",
      "Accuracy on incomplete training (22) data: 92.40%\n",
      "Accuracy on incomplete testing (22) data: 92.49%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m loss_details \u001b[38;5;241m=\u001b[39m train_loss_dict[loss_key]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Unpack the tuple returned by generate_random_data_removal\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m modified_train_data, _ \u001b[38;5;241m=\u001b[39m generate_random_data_removal(train_datasets, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming 1 is the loss percentage\u001b[39;00m\n\u001b[0;32m      7\u001b[0m modified_test_data, _ \u001b[38;5;241m=\u001b[39m generate_random_data_removal(test_datasets, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Proceed with data processing\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 28\u001b[0m, in \u001b[0;36mgenerate_random_data_removal\u001b[1;34m(data, percent_remove, total_plies)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Set the selected data points to None and record the indices in loss_dict\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(row_indices, col_indices):\n\u001b[1;32m---> 28\u001b[0m     modified_data\u001b[38;5;241m.\u001b[39miloc[row, col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m loss_dict:\n\u001b[0;32m     30\u001b[0m         loss_dict[col]\u001b[38;5;241m.\u001b[39mappend(row)\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 885\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1893\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1892\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1986\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1983\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1984\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m   1985\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[1;32m-> 1986\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_column(loc, value, pi)\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2095\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   2091\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39misetitem(loc, value)\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2093\u001b[0m     \u001b[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m     \u001b[38;5;66;03m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[1;32m-> 2095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mcolumn_setitem(loc, plane_indexer, value)\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1309\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[1;34m(self, loc, idx, value, inplace_only)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1308\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m col_mgr\u001b[38;5;241m.\u001b[39msetitem((idx,), value)\n\u001b[1;32m-> 1309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miset(loc, new_mgr\u001b[38;5;241m.\u001b[39m_block\u001b[38;5;241m.\u001b[39mvalues, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1112\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace, refs)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;66;03m# Accessing public blknos ensures the public versions are initialized\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m blknos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos[loc]\n\u001b[1;32m-> 1112\u001b[0m blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblklocs[loc]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m   1114\u001b[0m unfit_mgr_locs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1115\u001b[0m unfit_val_locs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:185\u001b[0m, in \u001b[0;36mBaseBlockManager.blklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rebuild_blknos_and_blklocs()\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blknos\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblklocs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]:\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    See blknos.__doc__\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# Note: these can be altered by other BlockManager methods.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Test pattern recognition on all losses for both train and test datasets\n",
    "\n",
    "### Note this will take a long time as it calculates the lost for 1% of data across 50 plies, suggest just running for 10 minutes\n",
    "### or so just to get an idea that the ouput is working.\n",
    "for loss_key in train_loss_dict.keys():\n",
    "    \n",
    "\n",
    "    # Unpack the tuple returned by generate_random_data_removal\n",
    "    modified_train_data, _ = generate_random_data_removal(train_datasets, 1)  # Assuming 1 is the loss percentage\n",
    "    modified_test_data, _ = generate_random_data_removal(test_datasets, 1)\n",
    "\n",
    "    # Proceed with data processing\n",
    "    processed_train_data = prepare_game_data_for_testing(modified_train_data)\n",
    "    processed_test_data = prepare_game_data_for_testing(modified_test_data)\n",
    "\n",
    "    # Pattern recognition and results storage\n",
    "    train_result_df = test_pattern_recognition_on_incomplete_data(mapper, processed_train_data, f\"training ({loss_key})\")\n",
    "    test_result_df = test_pattern_recognition_on_incomplete_data(mapper, processed_test_data, f\"testing ({loss_key})\")\n",
    "\n",
    "    results[loss_key] = {\n",
    "        'train': train_result_df,\n",
    "        'test': test_result_df\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Move_ply_1', 'Move_ply_2', 'Move_ply_3', 'Move_ply_4', 'Move_ply_5',\\n       'Move_ply_6', 'Move_ply_7', 'Move_ply_8', 'Move_ply_9', 'Move_ply_10',\\n       ...\\n       'Move_ply_191', 'Move_ply_192', 'Move_ply_193', 'Move_ply_194',\\n       'Move_ply_195', 'Move_ply_196', 'Move_ply_197', 'Move_ply_198',\\n       'Move_ply_199', 'Move_ply_200'],\\n      dtype='object', length=200)] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss, train_dataset \u001b[38;5;129;01min\u001b[39;00m train_datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m test_datasets[loss]\n\u001b[1;32m----> 5\u001b[0m     processed_train_data \u001b[38;5;241m=\u001b[39m prepare_game_data_for_testing(train_dataset)\n\u001b[0;32m      6\u001b[0m     processed_test_data \u001b[38;5;241m=\u001b[39m prepare_game_data_for_testing(test_dataset)\n\u001b[0;32m      8\u001b[0m     train_result_df \u001b[38;5;241m=\u001b[39m test_pattern_recognition_on_incomplete_data(mapper, processed_train_data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[76], line 16\u001b[0m, in \u001b[0;36mprepare_game_data_for_testing\u001b[1;34m(game_data, max_plies)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mPrepare the game data by creating move sequences from non-null moves.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m- pd.DataFrame: A DataFrame with a new column 'move_sequence' containing sequences of moves.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m move_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMove_ply_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_plies)]\n\u001b[1;32m---> 16\u001b[0m selected_moves_df \u001b[38;5;241m=\u001b[39m game_data[move_columns]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_move_sequence\u001b[39m(row):\n\u001b[0;32m     19\u001b[0m     moves \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Drop NaN values and convert to list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1072\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[1;32m-> 1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1113\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[1;32m-> 1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1517\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1518\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1520\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clancyam\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Move_ply_1', 'Move_ply_2', 'Move_ply_3', 'Move_ply_4', 'Move_ply_5',\\n       'Move_ply_6', 'Move_ply_7', 'Move_ply_8', 'Move_ply_9', 'Move_ply_10',\\n       ...\\n       'Move_ply_191', 'Move_ply_192', 'Move_ply_193', 'Move_ply_194',\\n       'Move_ply_195', 'Move_ply_196', 'Move_ply_197', 'Move_ply_198',\\n       'Move_ply_199', 'Move_ply_200'],\\n      dtype='object', length=200)] are in the [index]\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "139777\n"
     ]
    }
   ],
   "source": [
    "print(type(train_datasets))\n",
    "print(len(train_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Index', 'Date', 'ECO', 'Opening', 'Result', 'Termination',\n",
      "       'TimeControl', 'UTCDate', 'UTCTime', 'Move_ply_1',\n",
      "       ...\n",
      "       'Clock_ply_193', 'Clock_ply_194', 'Clock_ply_195', 'Clock_ply_196',\n",
      "       'Clock_ply_197', 'Clock_ply_198', 'Clock_ply_199', 'Clock_ply_200',\n",
      "       'Category', 'mapped_opening'],\n",
      "      dtype='object', length=411)\n"
     ]
    }
   ],
   "source": [
    "print(game_data.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
